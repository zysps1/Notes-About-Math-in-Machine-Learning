## 文献阅读（机器学习等）数学基础

<center>Yuanshuai</center>

[TOC]

### 一、线性代数

- 标量、向量

- 矩阵：$m\times n$

- 张量：标量、向量、矩阵的推广——>（0阶，1阶，2阶……）

  - 例如图像：长、宽、RGB值——>3阶张量

- 范数：描述向量大小

  - $$
    ||x||_p = (\sum_{i}|x_i|^p)^{\frac{1}{p}}
    $$

  - L1：p=1，向量绝对值之和

  - L2：平方和开方

- 特征分解：方阵A分解为一组特征向量（特征矩阵V）+特征值$\lambda$

  - $$
    Av = \lambda v
    $$

  - $$
    A = V diag(\lambda)V^{-1}
    $$

- 奇异值分解（Singular value decomposition,SVD)

  - 将普通矩阵分解为奇异向量和奇异值

  - $$
    A = UDV^T
    $$

  - 假设A为$m\times n$，那么 $U-m\times m, D-m\times n, V-n\times n$

  - 其中，U，V为正交矩阵，D为对角矩阵，D对角线上元素为A的奇异值，U的列向量为**左奇异向量**， V的列向量为**右奇异向量**

  - 可用于推荐系统中

- 伪逆（Moore-Penrose）

  - 对非方阵，根据矩阵A的左逆B求解方程

  - $$
    Ax = y
    $$

  - $$
    x  = By
    $$

  - 此时，若A行数>列数，则无解，反之有多解

  - 矩阵A 的伪逆计算

  - $$
    A^+ = VD^+U^T
    $$

  - 其中，$D^+$是其非零元素取倒之后再转置得到

- 常用距离——一般反映向量相似程度

  - 曼哈顿距离：对应元素差的绝对值之和

  - 欧氏距离：L2范数

  - 闵可夫斯基距离：类（对应元素差的）范数，有无数种可能

  - 契比雪夫距离：无穷范数（对应元素差的绝对值的最大值）

  - 夹角余弦：衡量样本向量之间的差异

  - $$
    cos\theta = \frac{AB}{|A||B|} = \frac{\sum^n_{k=1}x_{1k}x_{2k}}{\sqrt{\sum^n_{k=1}x_{1k}^2}\sqrt{\sum^n_{k=1}x_{2k}^2}}
    $$

  - 汉明距离：信息编码间（字符串间）不相同位数

  - 杰卡德相似系数：交集元素在并集中所占的比例

### 二、概率

- 随机变量

- 随机分布

- 条件概率

- $$
  P(Y = y|X = x) = \frac{P(Y=y,X=x)}{P(X=x)}
  $$

- $$
  P(B|A) = P(AB)/P(A)
  $$

- 贝叶斯公式：利用先验概率计算后验概率

- $$
  P(B_i|A) = \frac{P(A|B_i)\times P(B_i)}{\sum^N_{i=1}P(A|B_i)\times P(B_i)}
  $$

  - 理解：在检测疾病显示阳性的患者中某人真的患病的概率——后验概率

- 期望：离散求和，连续积分（平均水平）

- 方差：每个值与平均值的平方和的期望（与平均水平的偏离程度）

- 协方差：两个随机变量之间的总体误差

  - $$
    cov(X,Y) = E[XY]-E[X]E[Y]
    $$

- 常见分布函数

  - 0-1分布：p与1-p

  - 几何分布：n次伯努利实验中，k次才能第一次成功

    - $$
      P(X=k) = (1-p)^{k-1}p
      $$

    - $$
      E(X) = \frac{1-p}{p^2}
      $$

  - 二项分布：重复n次伯努利试验，各个实验相互独立，每次实验只有两种可能结果，且相互对立。某时间发生概率为p，则n次重复独立实验中发生k次的概率

    - $$
      P(X=k) = C_n^kp^k(1-p)^{n-k}
      $$

    - $$
      E(X) = np(1-p)
      $$

  - 高斯分布：

    - $$
      N(\mu,\sigma^2)
      $$

  - 指数分布：无记忆性，从开始算起至少t时间的概率。例如

    - 婴儿出生的时间间隔

    - 网站访问的时间间隔

    - 奶粉销售的时间间隔

    - $$
      P(X\le t) = 1-e^{-\lambda t}
      $$

  - 泊松分布：固定频率的事件，在某段时间内，事件具体的发生概率

    - $$
      P(N(t) = n) = \frac{(\lambda t)^n e^{-\lambda t}}{n!}
      $$

    - 表示在时间t内某个事件发生n次的概率，$\lambda$表示事件的固定频率

- Lagrange乘子法

  - 求满足一定约束条件的极值，把约束条件加到原函数上，对构造的新函数求导。

- 最大似然估计

  - 在“模型已定，参数未知”的情况下，通过观测数据估计未知参数

### 三、信息论

- 熵

  - $$
    H(X) = -\sum_{i=1}^n P(x_i)\log P(x_i) = \sum_{i=1}^n P(x_i) \frac{1}{\log P(x_i)}
    $$

- 联合熵

  - 二维随机变量XY的不确定性的度量

  - $$
    H(X,Y) = -\sum_{i=1}^n \sum_{j=1}^n P(x_i,y_i)\log P(x_i,y_i)
    $$

- 条件熵

  - 衡量已知随机变量X的条件下，随机变量Y的不确定性

  - $$
    H(Y|X) = -\sum_{x,y}P(x,y)\log P(y|x)
    $$

  - $$
    H(Y|X) = H(X,Y)-H(X)
    $$

- 相对熵（KL散度）

  - 互熵：描述两个概率分布差异。$D(P||Q)$表示当用概率分布Q来你和真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。

  - $$
    D(P||Q) = \sum_{i=1}^n P(x_i)\log \frac{P(x_i)}{Q(x_i)}
    $$

- 互信息

  - 一个随机变量里包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。
    $$
    I(X,Y) = \sum_{x\in X}\sum_{y\in Y}P(x,y)\log \frac{P(x,y)}{P(x)p(y)}
    $$

  - $$
    H(Y|X) = H(Y)-I(X,Y)
    $$

  - $$
    I(X,Y) = H(X)+H(Y)-H(X,Y)
    $$

- 最大熵模型

  - 在满足约束条件的模型集合中选取熵最大的模型

### 四、数值计算

- 上溢：无限值变为非数字

- 下溢：被四舍五入为0

  - 典型上下溢进行数值稳定的是softmax函数：在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1.

- 算法复杂性

- 确定性：针对自动机（基于状态变化进行迭代）模型，根据当时的状态和输入，自动机的状态转移是唯一确定的。

- 非确定性：在某一时刻自动机有多个状态可供选择，并尝试执行可选择的状态

- 非确定性算法容易陷入局部最优

- NP问题

  - P类问题：多项式时间的确定性算法可对问题判定或求解。算法中每个运行状态唯一，结果唯一最优。
  - NP问题：多项式时间的非确定性算法可对问题判定或求解，算法大多非确定性，但时间复杂度可能是多项式级别。
  - NP完全问题：任何一个问题至今都没有找到多项式时间的算法。
  - 机器学习中多数算法都是针对NP问题（包括NP完全问题）的

- 迭代计算

- 最优化问题

  - 变量、目标函数、约束条件

- 凸集：实数域R上的向量空间中，集合S中任两点的连线上的点都在S内，则S为凸集。

  - $$
    \lambda x+(1-\lambda)y \in S
    $$

  - 其中x,y为任意两点，$0\le \lambda \le 1$

- 超平面和半空间

  - 二维空间的超平面：一条线
  - 三维空间超平面：一个面
  - 半空间相对于超平面侧

- 凸集分离定理

  - 两个凸集合可以用一张超平面分割，无交叉重合

- 凸函数

  - 定义域在某个向量空间的凸子集上的实值函数

  - $$
    f(\theta x+(1-\theta)y)\le \theta f(x)+(1-\theta)f(y)
    $$

- 梯度下降算法

  - 求解无约束多元函数极值问题
  - 负梯度方向是f减小最快的方向

- 随机梯度下降

  - 动态步长取值，防止步长太大太小收敛过快过慢的问题
  - 随机去训练集中一部分样本梯度计算，避免有时陷入局部极小值
  - 相比批量梯度下降，随机梯度下降损失很小精度和增加一定数量迭代次数，提升总体优化效率。

- 牛顿法

  - 求解无约束最优化
  - 二阶收敛，比梯度下降（一阶）更快。

- 阻尼牛顿法

  - 牛顿法定步长迭代，不能保证函数值稳定下降，有时会发散
  - 每次迭代沿着迭代方向做一维搜索，寻求最优的步长因子

- 拟牛顿法

  - 简化牛顿法的计算
